{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olsem1/WP-3/blob/master/Article_DIK_2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWJWEJwNft2k"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEqEOrFzxl4s"
      },
      "source": [
        "from pmdarima.arima import auto_arima"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0elr5HmiI5a"
      },
      "source": [
        "# **Переменные**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atQOrSCziHzA"
      },
      "source": [
        "cpi = \"month\"\n",
        "reg_names = ['SFO']\n",
        "data_do = \"2020-12-01\"\n",
        "train_do = \"2019-01-01\"\n",
        "\n",
        "mes = (pd.to_datetime(data_do).to_period('M') - pd.to_datetime(train_do).to_period('M')).n+1\n",
        "result_rmse = pd.DataFrame(columns=['month/year', 'reg_names', 'data_do', 'train_do', 'mes', 'method', 'rmse'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BPbGQe9FZ8M"
      },
      "source": [
        "course = pd.read_excel('RC_2002-2020.xlsx')\n",
        "course = course.rename(columns={\"data\": \"date\"})\n",
        "course = course.set_index('date')\n",
        "course_m = course.resample('MS').mean()\n",
        "\n",
        "data = pd.read_excel('cpi_reg.xls', sheet_name = cpi)\n",
        "data = data.set_index('date')\n",
        "data = data.interpolate(method='polynomial', order=2, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwOYGv3_UiXk"
      },
      "source": [
        "# for ML\n",
        "data = data[reg_names]\n",
        "data = data[:data_do]\n",
        "data = data.merge(course_m, left_on='date', right_on='date')\n",
        "data['month'] = pd.DatetimeIndex(data.index).month\n",
        "\n",
        "train = data[data.index < train_do]\n",
        "test = data[data.index >= train_do]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Mmg2xE8jhm5"
      },
      "source": [
        "# **ARIMA**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV3tzZWSoa9n"
      },
      "source": [
        "# find best params for ARIMA\n",
        "stepwise_model = auto_arima(train[reg_names], test='adf', start_p=1, start_q=1, max_p=3, max_q=3, m=12, start_P=0, seasonal=True, \n",
        "                                D=1, d=1, max_d=4, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True, information_criterion = 'aic')\n",
        "\n",
        "stepwise_model.fit(train[reg_names])\n",
        "future_forecast = stepwise_model.predict(n_periods=test.shape[0])\n",
        "future_forecast = pd.DataFrame(future_forecast, index = test.index, columns=[reg_names])\n",
        "\n",
        "plt.plot(test[reg_names], label=\"test\")\n",
        "plt.plot(future_forecast[reg_names], label=\"forecast\")\n",
        "plt.legend()\n",
        "\n",
        "print('rmse: {:.4f}'.format(mean_squared_error(test[reg_names].values, future_forecast[reg_names].values) ** 0.5))\n",
        "print('normed rmse: {:.4f}%'.format(mean_squared_error(test[reg_names].values, future_forecast[reg_names].values) ** 0.5 / \n",
        "                                    np.mean(test[reg_names].values) * 100))\n",
        "\n",
        "result = pd.DataFrame(np.array(future_forecast), index = test.index, columns=['ARIMA'])\n",
        "result_arima = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'ARIMA', 'rmse': mean_squared_error(test[reg_names].values, future_forecast[reg_names].values) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_arima, ignore_index = True) \n",
        "result_rmse\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCbKOnHN-NbY"
      },
      "source": [
        "# **ARIMAX**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsBZpxzAbYK-"
      },
      "source": [
        "exogenous_features = ['month', 'curs']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UvwqI5NNCgJ"
      },
      "source": [
        "\n",
        "# find best params for ARIMAX\n",
        "stepwise_model = auto_arima(train[reg_names], exogenous = train[exogenous_features], test='adf', start_p=1, start_q=1, max_p=3, max_q=3, m=12, start_P=0, seasonal=True,\n",
        "                            d=1, D=1, trace=True, error_action='ignore', suppress_warnings=True, stepwise=True, information_criterion = 'aic')\n",
        " \n",
        "stepwise_model.fit(train[reg_names], exogenous = train[exogenous_features])\n",
        "future_forecast = stepwise_model.predict(n_periods=test.shape[0], exogenous = test[exogenous_features])\n",
        "future_forecast = pd.DataFrame(future_forecast, index = test.index, columns=[reg_names])\n",
        "\n",
        "plt.plot(test[reg_names], label=\"test\")\n",
        "plt.plot(future_forecast[reg_names], label=\"forecast\")\n",
        "plt.legend()\n",
        "\n",
        "print('rmse: {:.4f}'.format(mean_squared_error(test[reg_names].values, future_forecast[reg_names].values) ** 0.5))\n",
        "print('normed rmse: {:.4f}%'.format(mean_squared_error(test[reg_names].values, future_forecast[reg_names].values) ** 0.5 / \n",
        "                                    np.mean(test[reg_names].values) * 100))\n",
        "result['ARIMAX']=pd.DataFrame(np.array(future_forecast), index = test.index, columns=['ARIMAX'])\n",
        "result_arimax = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'ARIMAX', \n",
        "                'rmse': mean_squared_error(test[reg_names].values, future_forecast[reg_names].values) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_arimax, ignore_index = True) \n",
        "result_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5V9ronJuUjly"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bn2o2WJxoC9V"
      },
      "source": [
        "data_lags = pd.DataFrame()\n",
        "\n",
        "data_lags['curs'] = data['curs'].values\n",
        "data_lags['t'] = data[reg_names].values\n",
        "\n",
        "for i in [1, 6, 12]:\n",
        "    data_lags['t-'+str(i)] = data[reg_names].shift(i).values\n",
        "\n",
        "data_lags.index = data.index\n",
        "data_lags = data_lags[12:]\n",
        "data_lags['month'] = pd.DatetimeIndex(data_lags.index).month\n",
        "data_lags"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLQP6-DzVw_1"
      },
      "source": [
        "df=data_lags.copy()\n",
        "df.reset_index(drop=False, inplace=True)\n",
        "lag_features = [\"t\", \"curs\"]\n",
        "window1 = 3\n",
        "window2 = 6\n",
        "window3 = 12\n",
        "\n",
        "df_rolled_3m = df[lag_features].rolling(window=window1, min_periods=0)\n",
        "df_rolled_6m = df[lag_features].rolling(window=window2, min_periods=0)\n",
        "df_rolled_12m = df[lag_features].rolling(window=window3, min_periods=0)\n",
        "\n",
        "df_mean_3m = df_rolled_3m.mean().shift(1).reset_index().astype(np.float32)\n",
        "df_mean_6m = df_rolled_6m.mean().shift(1).reset_index().astype(np.float32)\n",
        "df_mean_12m = df_rolled_12m.mean().shift(1).reset_index().astype(np.float32)\n",
        "\n",
        "df_std_3m = df_rolled_3m.std().shift(1).reset_index().astype(np.float32)\n",
        "df_std_6m = df_rolled_6m.std().shift(1).reset_index().astype(np.float32)\n",
        "df_std_12m = df_rolled_12m.std().shift(1).reset_index().astype(np.float32)\n",
        "\n",
        "for feature in lag_features:\n",
        "    df[f\"{feature}_mean_lag{window1}\"] = df_mean_3m[feature]\n",
        "    df[f\"{feature}_mean_lag{window2}\"] = df_mean_6m[feature]\n",
        "    df[f\"{feature}_mean_lag{window3}\"] = df_mean_12m[feature]\n",
        "    \n",
        "    df[f\"{feature}_std_lag{window1}\"] = df_std_3m[feature]\n",
        "    df[f\"{feature}_std_lag{window2}\"] = df_std_6m[feature]\n",
        "    df[f\"{feature}_std_lag{window3}\"] = df_std_12m[feature]\n",
        "\n",
        "# df.fillna(df.mean(), inplace=True)\n",
        "df.dropna(axis=0, inplace=True)\n",
        "\n",
        "df.set_index(\"date\", drop=True, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uEM65WhngmI"
      },
      "source": [
        "data = df.copy()\n",
        "data_not = data.drop('t', axis=1)\n",
        "train = data[data.index < train_do]\n",
        "test = data[data.index >= train_do]\n",
        "\n",
        "X_train = train.drop(['t'], axis=1).values\n",
        "y_train = train[['t']].values.ravel()\n",
        "X_test = test.drop(['t'], axis=1).values\n",
        "y_test = test[['t']].values.ravel()\n",
        "\n",
        "X = data.drop(['t'], axis=1).values\n",
        "y = data[['t']].values.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqYEluZXTg6G"
      },
      "source": [
        "# **SCALER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ0ciSWIkq8M"
      },
      "source": [
        "# scale the data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train_m = scaler.transform(X_train)\n",
        "X_test_m = scaler.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73b5vMEVOGVd"
      },
      "source": [
        "result['y_test']=pd.DataFrame(np.array(y_test), index = test.index, columns=['y_test'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1ptDMz4gSvo"
      },
      "source": [
        "my_cv = TimeSeriesSplit(n_splits=16).split(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo6Hm7_ggS1a"
      },
      "source": [
        "for n, i in enumerate(my_cv):\n",
        "  print(i)\n",
        "  print(i[1].shape)\n",
        "  if n == 0:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYAfTf3Vp6On"
      },
      "source": [
        "# **Ridge (l2-regularisation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKuCK9tbhRam"
      },
      "source": [
        "np.linspace(0, 10, 21)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av93b7TsfMhu"
      },
      "source": [
        "%%time\n",
        "param_grid = {'alpha': np.linspace(0, 10, 21)}\n",
        "my_cv = TimeSeriesSplit(n_splits=16).split(X_train)\n",
        "\n",
        "GSRidge = GridSearchCV(Ridge(normalize='true'), param_grid, cv=my_cv)\n",
        "GSRidge.fit(X_train, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(GSRidge.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(GSRidge.score(X_test, y_test)))\n",
        "\n",
        "forecast = GSRidge.predict(X_test)\n",
        "print('rmse: {:.5f}'.format(mean_squared_error(y_test, forecast) ** 0.5))\n",
        "print('normed rmse: {:.2f}%'.format(mean_squared_error(y_test, forecast) ** 0.5 / np.mean(y_test) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQJcSD2SY4qv"
      },
      "source": [
        "result['Ridge']=pd.DataFrame(np.array(forecast), index = test.index, columns=['Ridge'])\n",
        "result_Ridge = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'Ridge', \n",
        "                'rmse': mean_squared_error(y_test, forecast) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_Ridge, ignore_index = True) \n",
        "result_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zMFKLQuvvb-"
      },
      "source": [
        "# **Lasso (l1-regularisation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCU4UKIHwKpo"
      },
      "source": [
        "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train_m, y_train)\n",
        "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train_m, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test_m, y_test)))\n",
        "print(\"Number of features used:\", np.sum(lasso001.coef_ != 0))\n",
        "\n",
        "forecast = lasso001.predict(X_test_m)\n",
        "print('rmse: {:.5f}'.format(mean_squared_error(y_test, forecast) ** 0.5))\n",
        "print('normed rmse: {:.2f}%'.format(mean_squared_error(y_test, forecast) ** 0.5 / np.mean(y_test) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3OH1F4zZFDS"
      },
      "source": [
        "result['Lasso']=pd.DataFrame(np.array(forecast), index = test.index, columns=['Lasso'])\n",
        "result_Lasso = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'Lasso', \n",
        "                'rmse': mean_squared_error(y_test, forecast) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_Lasso, ignore_index = True) \n",
        "result_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89Wk2X8YywhK"
      },
      "source": [
        "# **ElasticNet (l1+l2-regularisation)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bLvKIBNxmtY"
      },
      "source": [
        "from sklearn.linear_model import ElasticNet\n",
        "ENmodel = ElasticNet(alpha=0.1, max_iter=100000).fit(X_train, y_train)\n",
        "\n",
        "print(\"Training set score: {:.2f}\".format(ENmodel.score(X_train, y_train)))\n",
        "print(\"Test set score: {:.2f}\".format(ENmodel.score(X_test, y_test)))\n",
        "print(\"Number of features used:\", np.sum(ENmodel.coef_ != 0))\n",
        "\n",
        "forecast = ENmodel.predict(X_test)\n",
        "print('rmse: {:.5f}'.format(mean_squared_error(y_test, forecast) ** 0.5))\n",
        "print('normed rmse: {:.2f}%'.format(mean_squared_error(y_test, forecast) ** 0.5 / np.mean(y_test) * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-v_7zc8ZPc3"
      },
      "source": [
        "result['ElasticNet']=pd.DataFrame(np.array(forecast), index = test.index, columns=['ElasticNet'])\n",
        "result_ElasticNet = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'ElasticNet', \n",
        "                'rmse': mean_squared_error(y_test, forecast) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_ElasticNet, ignore_index = True) \n",
        "result_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VtIS_ts9gBH"
      },
      "source": [
        "# **Plot_features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uzl6UEi58e0C"
      },
      "source": [
        "def plot_features(model):\n",
        "  n_features = data_not.shape[1]\n",
        "  plt.barh(range(n_features), model.feature_importances_, align='center')\n",
        "  plt.yticks(np.arange(n_features), data_not.columns)\n",
        "  plt.xlabel('Важность признака')\n",
        "  plt.ylabel('Признак')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwJ1ETcy75nc"
      },
      "source": [
        "def plot_features_GS(model):\n",
        "  n_features = data_not.shape[1]\n",
        "  plt.barh(range(n_features), model.best_estimator_.feature_importances_, align='center')\n",
        "  plt.yticks(np.arange(n_features), data_not.columns)\n",
        "  plt.xlabel('Важность признака')\n",
        "  plt.ylabel('Признак')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZUBvN8MteZ3"
      },
      "source": [
        "# **GSRF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9VBj9kFtjX_"
      },
      "source": [
        "param_grid = {'n_estimators': [1, 10, 30, 50],\n",
        "              'max_depth' : [1, 3, 5, 7, 10]}\n",
        "\n",
        "my_cv = TimeSeriesSplit(n_splits=16).split(X_train)\n",
        "GSRF = GridSearchCV(RandomForestRegressor(), param_grid, cv=my_cv)\n",
        "GSRF.fit(X_train, y_train)\n",
        "for train_index, test_index in my_cv:\n",
        "  print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "  X_train, X_test = X[train_index], X[test_index]\n",
        "  y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "print(\"Best parameters: {}\".format(GSRF.best_params_))\n",
        "print(\"Best estimator:\\n{}\".format(GSRF.best_estimator_))\n",
        "\n",
        "# make predictions\n",
        "GSRF_forecast = GSRF.predict(X_test)\n",
        "\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(GSRF_forecast, label='forecast')\n",
        "plt.legend()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print('rmse: {:.4f}'.format(mean_squared_error(y_test, GSRF_forecast) ** 0.5))\n",
        "print('normed rmse: {:.4f}%'.format((mean_squared_error(y_test, GSRF_forecast) ** 0.5) / \n",
        "                                    np.mean(y_test) * 100))\n",
        "\n",
        "print(\"Accuracy on training set: {:.3f}\".format(GSRF.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(GSRF.score(X_test, y_test)))\n",
        "\n",
        "result['GSRF']=pd.DataFrame(np.array(GSRF_forecast), index = test.index, columns=['GSRF'])\n",
        "\n",
        "result_GSRF = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'GSRF', \n",
        "                'rmse': mean_squared_error(y_test, GSRF_forecast) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_GSRF, ignore_index = True) \n",
        "result_rmse\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2JybbWj8Boa"
      },
      "source": [
        "plot_features_GS(GSRF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjsuJnxA9pMB"
      },
      "source": [
        "# **Grid Search XGB**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNrnnv7HHq4G"
      },
      "source": [
        "param_grid = {'max_depth': [1, 2, 3, 5, 10],\n",
        "              'n_estimators': [1, 10, 50, 100, 200, 300, 500]}\n",
        "print(\"Parameter grid:\\n{}\".format(param_grid))\n",
        "my_cv = TimeSeriesSplit(n_splits=16).split(X_train)\n",
        "\n",
        "GSXGB = GridSearchCV(XGBRegressor(objective ='reg:squarederror'), param_grid, cv=my_cv)\n",
        "GSXGB.fit(X_train, y_train, verbose=False)\n",
        "\n",
        "print(\"Test set score: {:.2f}\".format(GSXGB.score(X_test, y_test)))\n",
        "print(\"Best parameters: {}\".format(GSXGB.best_params_))\n",
        "print(\"Best estimator:\\n{}\".format(GSXGB.best_estimator_))\n",
        "\n",
        "# make predictions\n",
        "GSXGB_forecast = GSXGB.predict(X_test)\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(GSXGB_forecast, label='forecast')\n",
        "plt.legend()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print('rmse: {:.4f}'.format(mean_squared_error(y_test, GSXGB_forecast) ** 0.5))\n",
        "print('normed rmse: {:.4f}%'.format((mean_squared_error(y_test, GSXGB_forecast) ** 0.5) / \n",
        "                                    np.mean(y_test) * 100))\n",
        "\n",
        "print(\"Accuracy on training set: {:.3f}\".format(GSXGB.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(GSXGB.score(X_test, y_test)))\n",
        "\n",
        "result['GSXGB']=pd.DataFrame(np.array(GSXGB_forecast), index = test.index, columns=['GSXGB'])\n",
        "\n",
        "result_GSXGB = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'GSXGB', \n",
        "                'rmse': mean_squared_error(y_test, GSXGB_forecast) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_GSXGB, ignore_index = True) \n",
        "result_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCXu7l58zRP-"
      },
      "source": [
        "def window_predict(model):  \n",
        "  data_for_window = data.reset_index(drop=True).copy()\n",
        "  # model = GSXGB\n",
        "\n",
        "  t_new = model.predict(data_for_window.iloc[-24].values[np.arange(len(X_test[0])+1) != 1].reshape(1,-1))\n",
        "  # print(t_new[0])\n",
        "\n",
        "  for i in range(24, 1, -1):\n",
        "    \n",
        "    data_for_window.at[len(data_for_window)-i, 't'] = t_new[0]\n",
        "\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't-1'] = data_for_window.at[len(data_for_window)-i-0, 't']\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't-6'] = data_for_window.at[len(data_for_window)-i-5, 't']\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't-12'] = data_for_window.at[len(data_for_window)-i-11, 't']\n",
        "\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't_mean_lag3'] = data_for_window.loc[(len(data_for_window)-i-2):(len(data_for_window)-i-0), 't'].mean()\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't_mean_lag6'] = data_for_window.loc[(len(data_for_window)-i-5):(len(data_for_window)-i-0), 't'].mean()\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't_mean_lag12'] = data_for_window.loc[(len(data_for_window)-i-11):(len(data_for_window)-i-0), 't'].mean()\n",
        "\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't_std_lag3'] = data_for_window.loc[(len(data_for_window)-i-2):(len(data_for_window)-i-0), 't'].std()\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't_std_lag6'] = data_for_window.loc[(len(data_for_window)-i-5):(len(data_for_window)-i-0), 't'].std()\n",
        "    data_for_window.at[len(data_for_window)-i+1, 't_std_lag12'] = data_for_window.loc[(len(data_for_window)-i-11):(len(data_for_window)-i-0), 't'].std()\n",
        "    # print(len(data_for_window)-i+1)\n",
        "\n",
        "    t_new = model.predict(data_for_window.iloc[-i+1].values[np.arange(len(X_test[0])+1) != 1].reshape(1,-1))\n",
        "    # print(-i+1)\n",
        "\n",
        "  return data_for_window.loc[len(data_for_window)-24:, 't'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH9nAOGcXD6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0526669-933a-4069-c2ed-8421e4e89001"
      },
      "source": [
        "window_predict(GSXGB)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([100.73468781, 100.52423859, 100.3842392 , 100.38998413,\n",
              "       100.4066391 , 100.40374756, 100.4094162 , 100.37042999,\n",
              "       100.38249969, 100.51979828, 100.57836914, 100.58583069,\n",
              "       100.46792603, 100.44301605, 100.45988464, 100.44806671,\n",
              "       100.44806671, 100.41181183, 100.43308258, 100.33538055,\n",
              "       100.36667633, 100.48823547, 100.52719879, 100.62      ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rj9_DLSbV8i2"
      },
      "source": [
        "# GSXGB_forecast = GSXGB.predict(X_test)\n",
        "GSXGB_forecast = window_predict(GSXGB)\n",
        "plt.plot(y_test, label='true')\n",
        "plt.plot(GSXGB_forecast, label='forecast')\n",
        "plt.legend()\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print('rmse: {:.4f}'.format(mean_squared_error(y_test, GSXGB_forecast) ** 0.5))\n",
        "print('normed rmse: {:.4f}%'.format((mean_squared_error(y_test, GSXGB_forecast) ** 0.5) / \n",
        "                                    np.mean(y_test) * 100))\n",
        "\n",
        "print(\"Accuracy on training set: {:.3f}\".format(GSXGB.score(X_train, y_train)))\n",
        "print(\"Accuracy on test set: {:.3f}\".format(GSXGB.score(X_test, y_test)))\n",
        "\n",
        "result['GSXGB_window']=pd.DataFrame(np.array(GSXGB_forecast), index = test.index, columns=['GSXGB_window'])\n",
        "\n",
        "result_GSXGB = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'GSXGB_window', \n",
        "                'rmse': mean_squared_error(y_test, GSXGB_forecast) ** 0.5} \n",
        "\n",
        "result_rmse = result_rmse.append(result_GSXGB, ignore_index = True) \n",
        "result_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpvS8RFm3rpw"
      },
      "source": [
        "# **Blend weight**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfXGdMvh3ub5"
      },
      "source": [
        "result_rmse1 = result_rmse[(result_rmse.method != 'ARIMA') & (result_rmse.method != 'ARIMAX')]\n",
        "\n",
        "result_rmse1['weight'] = 1/result_rmse1['rmse']/(1/result_rmse1['rmse']).sum()\n",
        "\n",
        "meta_X = result_rmse1['weight']\n",
        "meta_Y = result.drop(['y_test', 'ARIMA', 'ARIMAX'], axis=1)\n",
        "a = np.array(meta_X)\n",
        "b = np.array(meta_Y)\n",
        "meta_blend = (np.sum(a * b, axis = 1))\n",
        "\n",
        "result['Blend']=pd.DataFrame(np.array(meta_blend), index = test.index, columns=['Blend'])\n",
        "result_Blend = {'month/year': cpi, 'reg_names': reg_names, 'data_do': data_do, 'train_do': train_do, 'mes': mes, 'method': 'Blend', \n",
        "                'rmse': mean_squared_error(y_test, meta_blend) ** 0.5} \n",
        "result_rmse = result_rmse.append(result_Blend, ignore_index = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78uBokSDDWwR"
      },
      "source": [
        "# **Diebold-Mariano**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKvdguFEC2LS"
      },
      "source": [
        "def dm_test(actual_lst, pred1_lst, pred2_lst, h = 1, crit=\"MSE\", power = 2):\n",
        "    # Routine for checking errors\n",
        "    def error_check():\n",
        "        rt = 0\n",
        "        msg = \"\"\n",
        "        # Check if h is an integer\n",
        "        if (not isinstance(h, int)):\n",
        "            rt = -1\n",
        "            msg = \"The type of the number of steps ahead (h) is not an integer.\"\n",
        "            return (rt,msg)\n",
        "        # Check the range of h\n",
        "        if (h < 1):\n",
        "            rt = -1\n",
        "            msg = \"The number of steps ahead (h) is not large enough.\"\n",
        "            return (rt,msg)\n",
        "        len_act = len(actual_lst)\n",
        "        len_p1  = len(pred1_lst)\n",
        "        len_p2  = len(pred2_lst)\n",
        "        # Check if lengths of actual values and predicted values are equal\n",
        "        if (len_act != len_p1 or len_p1 != len_p2 or len_act != len_p2):\n",
        "            rt = -1\n",
        "            msg = \"Lengths of actual_lst, pred1_lst and pred2_lst do not match.\"\n",
        "            return (rt,msg)\n",
        "        # Check range of h\n",
        "        if (h >= len_act):\n",
        "            rt = -1\n",
        "            msg = \"The number of steps ahead is too large.\"\n",
        "            return (rt,msg)\n",
        "        # Check if criterion supported\n",
        "        if (crit != \"MSE\" and crit != \"MAPE\" and crit != \"MAD\" and crit != \"poly\"):\n",
        "            rt = -1\n",
        "            msg = \"The criterion is not supported.\"\n",
        "            return (rt,msg)  \n",
        "        # Check if every value of the input lists are numerical values\n",
        "        from re import compile as re_compile\n",
        "        comp = re_compile(\"^\\d+?\\.\\d+?$\")  \n",
        "        def compiled_regex(s):\n",
        "            \"\"\" Returns True is string is a number. \"\"\"\n",
        "            if comp.match(s) is None:\n",
        "                return s.isdigit()\n",
        "            return True\n",
        "        for actual, pred1, pred2 in zip(actual_lst, pred1_lst, pred2_lst):\n",
        "            is_actual_ok = compiled_regex(str(abs(actual)))\n",
        "            is_pred1_ok = compiled_regex(str(abs(pred1)))\n",
        "            is_pred2_ok = compiled_regex(str(abs(pred2)))\n",
        "            if (not (is_actual_ok and is_pred1_ok and is_pred2_ok)):  \n",
        "                msg = \"An element in the actual_lst, pred1_lst or pred2_lst is not numeric.\"\n",
        "                rt = -1\n",
        "                return (rt,msg)\n",
        "        return (rt,msg)\n",
        "    \n",
        "    # Error check\n",
        "    error_code = error_check()\n",
        "    # Raise error if cannot pass error check\n",
        "    if (error_code[0] == -1):\n",
        "        raise SyntaxError(error_code[1])\n",
        "        return\n",
        "    # Import libraries\n",
        "    from scipy.stats import t\n",
        "    import collections\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    \n",
        "    # Initialise lists\n",
        "    e1_lst = []\n",
        "    e2_lst = []\n",
        "    d_lst  = []\n",
        "    \n",
        "    # convert every value of the lists into real values\n",
        "    actual_lst = pd.Series(actual_lst).apply(lambda x: float(x)).tolist()\n",
        "    pred1_lst = pd.Series(pred1_lst).apply(lambda x: float(x)).tolist()\n",
        "    pred2_lst = pd.Series(pred2_lst).apply(lambda x: float(x)).tolist()\n",
        "    \n",
        "    # Length of lists (as real numbers)\n",
        "    T = float(len(actual_lst))\n",
        "    \n",
        "    # construct d according to crit\n",
        "    if (crit == \"MSE\"):\n",
        "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
        "            e1_lst.append((actual - p1)**2)\n",
        "            e2_lst.append((actual - p2)**2)\n",
        "        for e1, e2 in zip(e1_lst, e2_lst):\n",
        "            d_lst.append(e1 - e2)\n",
        "    elif (crit == \"MAD\"):\n",
        "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
        "            e1_lst.append(abs(actual - p1))\n",
        "            e2_lst.append(abs(actual - p2))\n",
        "        for e1, e2 in zip(e1_lst, e2_lst):\n",
        "            d_lst.append(e1 - e2)\n",
        "    elif (crit == \"MAPE\"):\n",
        "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
        "            e1_lst.append(abs((actual - p1)/actual))\n",
        "            e2_lst.append(abs((actual - p2)/actual))\n",
        "        for e1, e2 in zip(e1_lst, e2_lst):\n",
        "            d_lst.append(e1 - e2)\n",
        "    elif (crit == \"poly\"):\n",
        "        for actual,p1,p2 in zip(actual_lst,pred1_lst,pred2_lst):\n",
        "            e1_lst.append(((actual - p1))**(power))\n",
        "            e2_lst.append(((actual - p2))**(power))\n",
        "        for e1, e2 in zip(e1_lst, e2_lst):\n",
        "            d_lst.append(e1 - e2)    \n",
        "    \n",
        "    # Mean of d        \n",
        "    mean_d = pd.Series(d_lst).mean()\n",
        "    \n",
        "    # Find autocovariance and construct DM test statistics\n",
        "    def autocovariance(Xi, N, k, Xs):\n",
        "        autoCov = 0\n",
        "        T = float(N)\n",
        "        for i in np.arange(0, N-k):\n",
        "              autoCov += ((Xi[i+k])-Xs)*(Xi[i]-Xs)\n",
        "        return (1/(T))*autoCov\n",
        "    gamma = []\n",
        "    for lag in range(0,h):\n",
        "        gamma.append(autocovariance(d_lst,len(d_lst),lag,mean_d)) # 0, 1, 2\n",
        "    V_d = (gamma[0] + 2*sum(gamma[1:]))/T\n",
        "    DM_stat=V_d**(-0.5)*mean_d\n",
        "    harvey_adj=((T+1-2*h+h*(h-1)/T)/T)**(0.5)\n",
        "    DM_stat = harvey_adj*DM_stat\n",
        "    # Find p-value\n",
        "    p_value = 2*t.cdf(-abs(DM_stat), df = T - 1)\n",
        "    \n",
        "    # Construct named tuple for return\n",
        "    dm_return = collections.namedtuple('dm_return' , 'DM p_value')\n",
        "    \n",
        "    rt = dm_return(DM = DM_stat, p_value = p_value)\n",
        "    \n",
        "    return rt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vJxGf4qTHp2"
      },
      "source": [
        "actual_lst = result['y_test']\n",
        "pred1_lst = result['ARIMAX']\n",
        "pred2_lst = result['Blend']\n",
        "h = test.shape[0] - 1\n",
        "\n",
        "dm_test(actual_lst, pred1_lst, pred2_lst, h = h, crit=\"MSE\", power = 2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}